{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nr5IMiMiphP"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KY8fzYFgiphQ"
   },
   "outputs": [],
   "source": [
    "'''The model was adopted from a machine translation task, found in: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html'''\n",
    "'''The term *word* (in the code) represents *character* (in the code) while the term *sentence* represents a *word*'''\n",
    "'''It was decided to retain the original terminology of the code'''\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<\", 1: \">\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in list(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vx2ksDip96hl"
   },
   "outputs": [],
   "source": [
    "'''Reading Data'''\n",
    "\n",
    "def read_data(lang):\n",
    "  return pd.read_csv(str(lang).lower()+\"-train-medium\", sep=\"\\t\",header=None)\n",
    "\n",
    "# 'English' is set as the default selection out of this model's available languages.\n",
    "# To change the language, please select a different one from the following: Dutch, North-Frisian, Kannada or Polish.\n",
    "data = read_data(\"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6PfFYTqiphR"
   },
   "outputs": [],
   "source": [
    "'''Extracting and Preproceesing Data'''\n",
    "'''It was initially thought that only verbs and conjugations would be dealt with'''\n",
    "'''Later in the process it became clear that other lexical categories are also found in the data'''\n",
    "'''The term *verb* (in the code) could also refer to other lexical categories while the term *conjugation(s)* (in the code) refers to *inflection(s)*'''\n",
    "'''It was decided to retain the original terminology of the code'''\n",
    "\n",
    "V_morph = sorted(set(data[2]))\n",
    "morph2idx = {morph:idx for idx,morph in enumerate(V_morph)}\n",
    "\n",
    "\n",
    "def morph2idx_fun(el):\n",
    "    return morph2idx[el]\n",
    "\n",
    "\n",
    "data[3] = data[2].map(morph2idx_fun)\n",
    "X = data[[0,3]]\n",
    "y = data[1]\n",
    "\n",
    "keys = []\n",
    "values = []\n",
    "\n",
    "for idx in range(X.shape[0]):\n",
    "    keys.append((X.iloc[idx][0] + \"+\" + str(X.iloc[idx][3])))\n",
    "    values.append((y[idx]))\n",
    "    \n",
    "dicts = dict(zip(keys, values))\n",
    "X1 = pd.DataFrame(dicts.items())\n",
    "X1, X2 = train_test_split(X1, test_size = 0.2, random_state = 42) # X1 would be used for training while X2 would be used for testing.\n",
    "MAX_LENGTH = max(max(X1[1].map(len)), max(X2[1].map(len)))+2\n",
    "X1 = X1.reset_index()[[0, 1]]\n",
    "X2 = X2.reset_index()[[0, 1]]\n",
    "X1 = X1.rename(columns={0: 'Verb', 1: 'Conjugation'})\n",
    "X2 = X2.rename(columns={0: 'Verb', 1: 'Conjugation'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hJYXREfiphY"
   },
   "outputs": [],
   "source": [
    "'''One-hot-encoding the Data'''\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = []\n",
    "target_characters = []\n",
    "\n",
    "for idx in range(len(X1['Verb'])):\n",
    "    input_text = X1.loc[idx, 'Verb']\n",
    "    target_text = X1.loc[idx, 'Conjugation']\n",
    "    # I use \"tab\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in list(input_text):\n",
    "        if char not in input_characters:\n",
    "            input_characters.append(char)\n",
    "    for char in list(target_text):\n",
    "        if char not in target_characters:\n",
    "            target_characters.append(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "\n",
    "max_encoder_seq_length = len(X1['Verb'])\n",
    "max_decoder_seq_length = len(X1['Conjugation'])\n",
    "\n",
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "val_list = []\n",
    "for val in input_token_index.values():\n",
    "    val_list.append(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kf1R_YaEiphZ"
   },
   "outputs": [],
   "source": [
    "def readLangs(input_data, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    lang1 = \"lemma_morph\"\n",
    "    lang2 = \"surface\"\n",
    "\n",
    "    pairs = input_data.values.tolist()\n",
    "\n",
    "    '''Reversing pairs and making Lang instances'''\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pibBpvsaiphZ"
   },
   "outputs": [],
   "source": [
    "def prepareData(input_data, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(input_data, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(X1, False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NOJuPF8ipha"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkfJedO4ipha"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkOXbiIGipha"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IO2FMD31iphb"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in list(sentence)]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiGTlIeZiphb"
   },
   "outputs": [],
   "source": [
    "def MED(sent_01, sent_02):\n",
    "    n = len(sent_01)\n",
    "    m = len(sent_02)\n",
    "\n",
    "    matrix = [[i+j for j in range(m+1)] for i in range(n+1)]\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            if sent_01[i-1] == sent_02[j-1]:\n",
    "                d = 0\n",
    "            else:\n",
    "                d = 1\n",
    "\n",
    "            matrix[i][j] = min(matrix[i-1][j]+1, matrix[i][j-1]+1, matrix[i-1][j-1]+d)\n",
    "\n",
    "    distance_score = matrix[n][m]\n",
    "   \n",
    "    return distance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5u-1jNMiphb"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaGML5Vpiphb"
   },
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jssw2cSiiphb"
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=10000, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MG8FS6sIiphc"
   },
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # This locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJ4ZE_lliphc"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJApsukwiphc"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('MED = ', MED((str(pair[1]).replace(\" \", \"\")), str(output_sentence.replace(\" \", \"\").replace(\">\", \"\"))))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Uaeqm_oiphc"
   },
   "outputs": [],
   "source": [
    "'''The hyperparametes: hidden_size, dropout_p, num_iters, print_every and l_rate were set to default values and may be changed'''\n",
    "hidden_size = 512\n",
    "num_iters = 100000\n",
    "l_rate = 0.001\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, num_iters, print_every=10000, learning_rate=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shkB01cbiphd"
   },
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDrgpdcgiphd"
   },
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + list(input_sentence) +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence, gold_sent):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    print('MED = ', MED(gold_sent.replace(\" \", \"\"), (' '.join(output_words).replace(\" \", \"\").replace(\">\", \"\"))))\n",
    "    showAttention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_vxgt8Eiphe"
   },
   "outputs": [],
   "source": [
    "'''Preparing the testing data including preprocessing'''\n",
    "\n",
    "def has_any(word, presence_dict):\n",
    "    new_dict = presence_dict\n",
    "    for k in new_dict.keys():\n",
    "        if k in word:\n",
    "            new_dict[k] = True\n",
    "        else:\n",
    "            new_dict[k] = False\n",
    "            \n",
    "    return new_dict\n",
    "\n",
    "input_lang2, _, _ = prepareData(X2, False)\n",
    "x1_dict = copy.deepcopy(input_lang.word2index)\n",
    "x2_dict = copy.deepcopy(input_lang2.word2index)\n",
    "comb_dict = {**x1_dict, **x2_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2yuAe-ziphf"
   },
   "outputs": [],
   "source": [
    "def return_false_keys1(presence_dict):\n",
    "    false_keys_list = []\n",
    "    for key, value in comb_dict.items():\n",
    "        if key not in list(presence_dict.keys()):\n",
    "            false_keys_list.append(key)\n",
    "    return false_keys_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4IwOpbciphf"
   },
   "outputs": [],
   "source": [
    "keys_not_in_x1 = return_false_keys1(x1_dict)\n",
    "keys_not_in_x2 = return_false_keys1(x2_dict)\n",
    "\n",
    "print(\"The following characters appear in the test set but not in the training set:\")\n",
    "print(keys_not_in_x1)\n",
    "print()\n",
    "print(\"The following characters appear in the training set but not in the test set:\")\n",
    "print(keys_not_in_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ir6tHbVfiphg"
   },
   "outputs": [],
   "source": [
    "'''Finding unknown verb conjugations (or inflections to be exact)'''\n",
    "\n",
    "def check_false_chars(word):\n",
    "    for char in keys_not_in_x1:\n",
    "        if char in word:\n",
    "            return None\n",
    "    \n",
    "    return 1\n",
    "\n",
    "X2['UNKVC'] = X2.Verb.apply(check_false_chars)\n",
    "X2['UNKCC'] = X2.Conjugation.apply(check_false_chars)\n",
    "\n",
    "X2.dropna(subset = ['UNKVC'], inplace=True)\n",
    "print(\"A total of \", 200-len(X2), \" entries have been removed from the original test-set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3ilpy_7iphh"
   },
   "outputs": [],
   "source": [
    "'''Preparing the final version of the test-set after preprocessing'''\n",
    "\n",
    "X2 = X2.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "x_test = X2['Verb']\n",
    "y_test = X2['Conjugation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzM1jDj2iphh"
   },
   "outputs": [],
   "source": [
    "'''Uncomment the following to see attention alignment visualisations'''\n",
    "\n",
    "# for idx, el in enumerate(x_test):\n",
    "#     evaluateAndShowAttention(x_test[idx], y_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obdJDqaGiphh"
   },
   "outputs": [],
   "source": [
    "sum_med = 0\n",
    "acc_count = 0\n",
    "\n",
    "for idx, el in enumerate(x_test):\n",
    "    out, _ = evaluate(encoder1, attn_decoder1, x_test[idx])\n",
    "    out = ' '.join(out).replace(\" \", \"\").replace(\">\", \"\")\n",
    "    sum_med+=(MED(out, y_test[idx].replace(\" \", \"\")))\n",
    "    if (MED(out, y_test[idx])) == 0:\n",
    "        acc_count+=1    \n",
    "    \n",
    "print(\"The measured average MED score is: \", sum_med/len(x_test))\n",
    "print(\"The measured accuracy score is: \", acc_count/len(x_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Free Topic Exam Project Codebook E21 (Tom Yagel, tjw403).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
